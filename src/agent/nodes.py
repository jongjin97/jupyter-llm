import nbformat
from nbformat.v4 import new_code_cell, new_output
from .state import AgentState
from pydantic import BaseModel, Field
# from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from typing import List, Literal
from src.tools.jupyter_executor import JupyterExecutor


class SuggestedOptions(BaseModel):
    """A list of suggested next steps for the user to choose from."""
    options: List[str] = Field(description="A concise list of 3-5 logical next steps.")

class CodePlan(BaseModel):
    code: str = Field(description="Jupyter ì»¤ë„ì—ì„œ ì‹¤í–‰í•  ë‹¨ì¼ Python ì½”ë“œ ë¸”ë¡.")
    reasoning: str = Field(description="ì´ ì½”ë“œê°€ ì£¼ì–´ì§„ ì‘ì—…ì„ ì–´ë–»ê²Œ ìˆ˜í–‰í•˜ëŠ”ì§€ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª….")

class Route(BaseModel):
    """The routing decision for the user's task."""
    destination: Literal["simple_task", "complex_task"] = Field(description="The destination to route to based on task complexity.")
    task_type: Literal["file_system", "data_analysis", "visualization", "ml_engineering", "general"] = Field(description="The specific expertise required for the task.")

class ErrorDecision(BaseModel):
    """stderr ë‚´ìš©ì´ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
    is_critical_error: bool = Field(
        description="True: Traceback, SyntaxError, NameError ë“± ì½”ë“œë¥¼ ìˆ˜ì •í•´ì•¼ í•˜ëŠ” ì¹˜ëª…ì ì¸ ì˜¤ë¥˜. False: [notice]ë‚˜ pip ì—…ë°ì´íŠ¸ ì•Œë¦¼ì²˜ëŸ¼ ë¬´ì‹œí•´ë„ ë˜ëŠ” ê²½ê³  ë˜ëŠ” ë¹ˆ ë¬¸ìì—´."
    )

def router_node(state: AgentState) -> dict:
    """
    [ì—­í• : ì´ê´„ ë§¤ë‹ˆì €]
    ì‚¬ìš©ìì˜ ì‘ì—…ì„ ë¶„ì„í•˜ì—¬ 'ë‹¨ìˆœ/ë³µì¡' ì—¬ë¶€ì™€ í•„ìš”í•œ 'ì „ë¬¸ê°€ ìœ í˜•'ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "You are an expert at classifying user requests for a Python coding agent. "
         "First, determine if the task is 'simple_task' (can be done in one obvious step) or 'complex_task' (is vague and needs user feedback). "
         "Second, classify the task into one of the following expertise types: "
         "- 'file_system': For tasks involving file or directory listing, reading, writing (os, glob, pathlib). "
         "- 'data_analysis': For tasks involving data manipulation, cleaning, and analysis (pandas, numpy). "
         "- 'visualization': For tasks involving plotting and creating charts (matplotlib, seaborn). "
         "- 'ml_engineering': For tasks involving machine learning model training and evaluation (scikit-learn). "
         "- 'general': For any other general Python coding task. "
         "Respond with both the destination and the task_type."),
        ("human", "User's task: {task}")
    ])
    llm = ChatOpenAI(model="gpt-5-mini", temperature=0)
    structured_llm = llm.with_structured_output(Route)

    route = structured_llm.invoke(prompt.format(task=state["task"]))

    # ë‹¤ìŒ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. LangGraphëŠ” ì´ ê°’ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ê¸°í•©ë‹ˆë‹¤.
    return {"destination": route.destination, "task_type": route.task_type}

def option_suggester_node(state: AgentState) -> dict:
    """
    í˜„ì¬ ìƒíƒœ(ë…¸íŠ¸ë¶ ë‚´ìš©, ê³¼ê±° ê¸°ë¡ í¬í•¨)ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬
    ì‚¬ìš©ìì—ê²Œ ë‹¤ìŒì— ìˆ˜í–‰í•  ì‘ì—… ì„ íƒì§€ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤.
    """
    # âœ¨ --- ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ ---
    # plannerì²˜ëŸ¼, ì œì•ˆì„ ìœ„í•´ì„œë„ ì¶©ë¶„í•œ ë§¥ë½ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.

    # ìµœê·¼ ì…€ ë‚´ìš© ì¶”ì¶œ
    notebook_data = state.get("notebook")
    if isinstance(notebook_data, dict):
        notebook = nbformat.from_dict(notebook_data)
    else:
        notebook = notebook_data # ì´ë¯¸ ê°ì²´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
    recent_cells_source = []
    if notebook and notebook.cells:
        num_recent_cells = 5
        for cell in notebook.cells[-num_recent_cells:]:
            if cell.cell_type == 'code':
                recent_cells_source.append(f"# Previous Code Cell:\n{cell.source}")
    formatted_recent_cells = "\n---\n".join(recent_cells_source)

    # ê³¼ê±° ì‘ì—… ë‚´ì—­ ìš”ì•½
    formatted_history = "\n---\n".join(state.get("history", []))

    # í”„ë¡¬í”„íŠ¸ì— ìˆ˜ì§‘í•œ ëª¨ë“  ë§¥ë½ ì •ë³´ë¥¼ í¬í•¨ì‹œí‚µë‹ˆë‹¤.
    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "You are a helpful data analysis assistant. Your job is to look at the current state of the analysis "
         "and suggest a list of logical next steps for the user to choose from. "
         "Review the user's request, the recent notebook cells, and the history to make relevant suggestions. "
         "Provide a concise list of 3-5 actionable options."),
        ("human",
         "--- User's Overall Task ---\n"
         "{task}\n\n"
         "--- Recent Notebook Cells ---\n"
         "{recent_cells}\n\n"
         "--- Analysis History ---\n"
         "{history}\n\n"
         "Based on all the information above, what are the best next steps for the user to choose from? Respond with a list of options.")
    ])

    llm = ChatOpenAI(model="gpt-5-mini", temperature=0)
    structured_llm = llm.with_structured_output(SuggestedOptions)

    response = structured_llm.invoke(prompt.format(
        task=state['task'],
        recent_cells=formatted_recent_cells,
        history=formatted_history
    ))

    return {"suggested_options": response.options}


def code_generator_node(state: AgentState) -> dict:
    """
    ì‚¬ìš©ìê°€ ì„ íƒí•œ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹¨ì¼ ì‘ì—…ì„ Python ì½”ë“œë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    # 1. ìƒíƒœì—ì„œ í•„ìš”í•œ ëª¨ë“  ë§¥ë½ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    #    ì´ì œ 'task'ëŠ” "ê²°ì¸¡ì¹˜ í™•ì¸"ê³¼ ê°™ì´ ë§¤ìš° êµ¬ì²´ì ì¸ ëª…ë ¹ì…ë‹ˆë‹¤.
    task = state["task"]
    notebook_data = state.get("notebook")

    stdout = state.get("stdout", "")
    stderr = state.get("stderr", "")

    # ì²´í¬í¬ì¸í„°ê°€ ê°ì²´ë¥¼ dictë¡œ ë³€í™˜í–ˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë‹¤ì‹œ NotebookNode ê°ì²´ë¡œ ë³µì›í•©ë‹ˆë‹¤.
    if isinstance(notebook_data, dict):
        notebook = nbformat.from_dict(notebook_data)
    else:
        notebook = notebook_data # ì´ë¯¸ ê°ì²´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©

    # ìµœê·¼ ì…€ ë‚´ìš© ì¶”ì¶œ
    recent_cells_source = []
    if notebook and notebook.cells:
        num_recent_cells = 5
        for cell in notebook.cells[-num_recent_cells:]:
            if cell.cell_type == 'code':
                recent_cells_source.append(f"# Previous Code Cell:\n{cell.source}")
    formatted_recent_cells = "\n---\n".join(recent_cells_source)

    # ê³¼ê±° ì‘ì—… ë‚´ì—­ ìš”ì•½
    formatted_history = "\n---\n".join(state.get("history", []))

    # ì „ë¬¸ê°€ ëª¨ë“œ ê²°ì •
    task_type = state.get("task_type", "general")

    # ê° ì „ë¬¸ê°€ ëª¨ë“œì— ë§ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
    expert_prompts = {
        "file_system": "You are a Python expert specializing in file system operations. Use `os`, `glob`, and `pathlib` to handle file and directory tasks efficiently and safely.",
        "data_analysis": "You are a senior data analyst. Your expertise is in using `pandas` and `numpy` for data manipulation, cleaning, aggregation, and analysis. Always aim for idiomatic pandas code.",
        "visualization": "You are a data visualization specialist. Use `matplotlib` and `seaborn` to create clear and insightful charts. **CRITICAL: You MUST execute `%matplotlib inline` before any plotting commands.**",
        "ml_engineering": "You are a machine learning engineer. Your specialty is using `scikit-learn` to build preprocessing pipelines, train models, and evaluate their performance. Use standard variable names like `X_train`, `y_train`.",
        "general": "You are a general-purpose, highly skilled Python code generation tool. Write clean, efficient, and correct Python code to accomplish the given task."
    }

    # ì„ íƒëœ ì „ë¬¸ê°€ ëª¨ë“œì— ë§ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    system_prompt = expert_prompts.get(task_type, expert_prompts["general"])
    # ì—­í• ì´ ë‹¨ìˆœí™”ëœ ë§Œí¼, í”„ë¡¬í”„íŠ¸ë„ í›¨ì”¬ ë” ëª…í™•í•˜ê³  ê°„ê²°í•´ì§‘ë‹ˆë‹¤.
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system",
             f"{system_prompt}"
             "\n\n--- YOUR WORKFLOW & RULES ---\n"
             "1. **Analyze & Plan:** Review all context and the `Task To Execute Now`."
             "2. **Code Generation:** Write the Python code to accomplish the task."
             "3. **Self-Testing (CRITICAL):** After writing the main logic (like a function or a complex transformation), you MUST add a few lines of simple test code (`assert` or `print` checks) to verify that your code works as expected. This helps catch errors early."
             "   - *Example:* If you create a function `def add(a, b): ...`, you should add `assert add(3, 5) == 8` afterwards."
             "4. **Error Handling:** If the previous step had an error (`STDERR` is not empty), your only goal is to fix that error."
             "\n\n--- OTHER RULES ---\n"
             " - If a library is needed, `!pip install` it."
             " - If you need to plot, execute `%matplotlib inline` first."),
            ("human",
             "--- Context: Recent Notebook Cells ---\n"
             "{recent_cells}\n\n"
             "--- Context: History of Past Actions ---\n"
             "{history}\n\n"
             # âœ¨ ìˆ˜ì •ëœ ë¶€ë¶„: ì§ì „ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì „ë‹¬í•˜ëŠ” ì„¹ì…˜ ì¶”ê°€
             "--- Context: Result of Last Execution ---\n"
             "STDOUT:\n{stdout}\n\n"
             "STDERR:\n{stderr}\n\n"
             "--- **Task To Execute Now** ---\n"
             "**{task}**\n\n"
             "Please write the single block of Python code to perform your task based on your workflow.")
        ]
    )

    # 3. LLMì„ í˜¸ì¶œí•˜ì—¬ ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    llm = ChatOpenAI(model="gpt-5-mini", temperature=0)
    structured_llm = llm.with_structured_output(CodePlan)

    response = structured_llm.invoke(prompt.format(
        task=task,
        recent_cells=formatted_recent_cells,
        history=formatted_history,
        stdout=stdout,
        stderr=stderr
    ))

    # 4. ìƒì„±ëœ ì½”ë“œë¥¼ 'plan'ìœ¼ë¡œ ë°˜í™˜í•˜ì—¬ executorì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤.
    return {"plan": [response.code]}


def code_executor_node(state: AgentState, executor: JupyterExecutor):
    """
    ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³ , ì‹¤í–‰ ë‚´ì—­ì„ ë…¸íŠ¸ë¶ì— ê¸°ë¡í•œ ë’¤ ì €ì¥í•©ë‹ˆë‹¤.
    """
    code_to_run = state['plan'][-1]

    if code_to_run == "FINISH":
        return {"executed_code": "FINISH", "stdout": "Task completed."}

    # 1. ìƒíƒœì—ì„œ ë…¸íŠ¸ë¶ ê°ì²´ì™€ ê²½ë¡œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    notebook_data = state['notebook']

    if isinstance(notebook_data, dict):
        notebook = nbformat.from_dict(notebook_data)
    else:
        notebook = notebook_data

    notebook_path = state['notebook_path']

    # 2. ë…¸íŠ¸ë¶ì— ìƒˆë¡œìš´ ì½”ë“œ ì…€ì„ ì¶”ê°€í•©ë‹ˆë‹¤. (ê¸°ë¡)
    cell = new_code_cell(code_to_run)
    notebook.cells.append(cell)

    # 3. ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    # executor = state['kernel_executor']
    result = executor.execute(code_to_run)

    # stdout ê²°ê³¼ê°€ ì‡ë‹¤ë©´, name='stdout'ì¸ stream ê°ì²´ë¥¼ ë§Œë“¤ì–´ ì¶”ê°€
    if result['stdout']:
        stdout_output = new_output(
            output_type="stream",
            name="stdout",
            text=result['stdout']
        )
        cell.outputs.append(stdout_output)

    # ë¦¬ì¹˜ ì¶œë ¥(ì´ë¯¸ì§€ ë“±)ì„ ì¶”ê°€
    if result['outputs']:

        for output_content in result['outputs']:
            # nbformatì´ ìš”êµ¬í•˜ëŠ” 'data', 'metadata' í˜•ì‹ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
            cell.outputs.append(new_output(
                output_type=output_content.get('header', {}).get('msg_type', 'display_data'),
                data=output_content.get('data', {}),
                metadata=output_content.get('metadata', {})
            ))

    # stderr ê²°ê³¼ê°€ ìˆë‹¤ë©´, name='stderr'ì¸ stream ê°ì²´ë¥¼ ë§Œë“¤ì–´ ì¶”ê°€
    if result['stderr']:
        stderr_output = new_output(
            output_type="stream",
            name="stderr",
            text=result['stderr']
        )
        cell.outputs.append(stderr_output)

    # ì¶”ê°€ ì‹¤í–‰ ê²°ê³¼ë¥¼ 'Output' ê°ì²´ë¡œ ë§Œë“­ë‹ˆë‹¤.
    # ê°€ì¥ ì¼ë°˜ì ì¸ 'stream' íƒ€ì…ì˜ ì¶œë ¥ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    # output = new_output(
    #     output_type="stream",
    #     name="stdout", # í‘œì¤€ ì¶œë ¥
    #     text=observation, # executorê°€ ë°˜í™˜í•œ ê²°ê³¼ ë¬¸ìì—´
    # )
    # cell.outputs.append(output)
    # 4. ë³€ê²½ëœ ë…¸íŠ¸ë¶ ê°ì²´ë¥¼ íŒŒì¼ì— ë‹¤ì‹œ ì”ë‹ˆë‹¤. (ì €ì¥)
    try:
        with open(notebook_path, 'w', encoding='utf-8') as f:
            nbformat.write(notebook, f)
    except Exception as e:
        result["stderr"] += f"\n\nê²½ê³ : ë…¸íŠ¸ë¶ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ - {e}"

    history = state.get("history", [])
    summary = f"Executed Code:\n```python\n{code_to_run}\n```\n\nSTDOUT:\n{result['stdout']}\n\nSTDERR:\n{result['stderr']}"
    history.append(summary)

    # 5, ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    return {
        "executed_code": code_to_run,
        "stdout": result["stdout"],
        "stderr": result["stderr"],
        "notebook": notebook,
        "history": history
    }

def error_classifier_node(state: AgentState) -> dict:
    """
    [Node] AI ê¸°ë°˜ì˜ ì˜¤ë¥˜ ë¶„ë¥˜ê¸° (AI ì‹¬íŒ)
    'stderr'ì™€ 'ì‹¤í–‰ëœ ì½”ë“œ'ë¥¼ í•¨ê»˜ ë¶„ì„í•˜ì—¬,
    ì´ê²ƒì´ ì½”ë“œë¥¼ ìˆ˜ì •í•´ì•¼ í•˜ëŠ” 'ì¹˜ëª…ì ì¸ ì˜¤ë¥˜'ì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤.
    """
    stderr = state.get("stderr", "")
    executed_code = state.get("executed_code", "")  # ì‹¤í–‰ëœ ì½”ë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.

    # í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ LLMì—ê²Œ ëª…í™•í•œ íŒë‹¨ ê¸°ì¤€ì„ ì œì‹œí•©ë‹ˆë‹¤.
    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "You are an expert error classifier. Your job is to analyze an error log (STDERR) *and* the code that produced it. "
         "You must decide if the error is a CRITICAL, code-breaking error that requires fixing the code, or an IGNORABLE warning."
         "\n\nCRITICAL errors include: Traceback, SyntaxError, NameError, TypeError, FileNotFoundError, etc."
         "\nIGNORABLE warnings include: '[notice]', 'A new release of pip is available', deprecation warnings, etc."
         "\nIf STDERR is empty, it is not a critical error."),
        ("human",
         "--- EXECUTED CODE ---\n"
         "```python\n{code}\n```\n\n"
         "--- STDERR ---\n{stderr}\n\n"
         "Is this a critical error that requires fixing the code? Respond with boolean 'is_critical_error' only.")
    ])

    llm = ChatOpenAI(model="gpt-5-mini", temperature=0)  # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ì„¤ì •
    structured_llm = llm.with_structured_output(ErrorDecision)

    # âœ¨ invoke í˜¸ì¶œ ì‹œ executed_codeë¥¼ í•¨ê»˜ ì „ë‹¬
    decision = structured_llm.invoke(prompt.format(
        code=executed_code,
        stderr=stderr
    ))

    if decision.is_critical_error:
        print("ğŸ”¥ AIê°€ ì‹¬ê°í•œ ì˜¤ë¥˜ë¥¼ ê°ì§€í–ˆìŠµë‹ˆë‹¤. ìˆ˜ì •ì„ ìœ„í•´ generatorë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.")
        return {"destination": "fix_error"}
    else:
        return {"destination": "no_error"}