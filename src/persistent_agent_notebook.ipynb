{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7c176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data files in C:\\Users\\kakao\\PycharmProjects\\jupyter-llm\\src (non-recursive):\n",
      "   C:\\Users\\kakao\\PycharmProjects\\jupyter-llm\\src\\titanic.xls\n",
      "Self-tests passed."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "# Set of common data file extensions to look for\n",
    "DATA_EXTENSIONS = {'.csv', '.json', '.xlsx', '.xls', '.parquet', '.txt', '.tsv',\n",
    "                   '.xml', '.yaml', '.yml', '.h5', '.hdf5', '.pkl', '.ndjson'}\n",
    "\n",
    "\n",
    "def list_data_files(directory='.', recursive=False, include_hidden=False):\n",
    "    \"\"\"Return a sorted list of data files in `directory`.\n",
    "\n",
    "    Parameters:\n",
    "    - directory: path-like (str or Path). Defaults to current directory.\n",
    "    - recursive: if True, search subdirectories recursively.\n",
    "    - include_hidden: if False (default), skip files or path parts that start with a dot.\n",
    "\n",
    "    Returns: list of string paths (absolute).\n",
    "    \"\"\"\n",
    "    p = Path(directory)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Directory not found: {directory}\")\n",
    "    files = []\n",
    "\n",
    "    if recursive:\n",
    "        # rglob through all files and filter by extension\n",
    "        for path in p.rglob('*'):\n",
    "            if not path.is_file():\n",
    "                continue\n",
    "            ext = path.suffix.lower()\n",
    "            if ext not in DATA_EXTENSIONS:\n",
    "                continue\n",
    "            if not include_hidden:\n",
    "                # skip if any path component is hidden\n",
    "                if any(part.startswith('.') for part in path.parts):\n",
    "                    continue\n",
    "            files.append(str(path.resolve()))\n",
    "    else:\n",
    "        for path in p.iterdir():\n",
    "            if not path.is_file():\n",
    "                continue\n",
    "            ext = path.suffix.lower()\n",
    "            if ext not in DATA_EXTENSIONS:\n",
    "                continue\n",
    "            if not include_hidden and path.name.startswith('.'):\n",
    "                continue\n",
    "            files.append(str(path.resolve()))\n",
    "\n",
    "    return sorted(files)\n",
    "\n",
    "\n",
    "# Show data files in the current directory (non-recursive) when run\n",
    "if __name__ == '__main__':\n",
    "    cwd = Path('.').resolve()\n",
    "    print(f\"Data files in {cwd} (non-recursive):\")\n",
    "    found = list_data_files(cwd, recursive=False, include_hidden=False)\n",
    "    if not found:\n",
    "        print(\"  (none found)\")\n",
    "    else:\n",
    "        for f in found:\n",
    "            print(\"  \", f)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# Self-tests using a temporary directory to avoid touching user's files\n",
    "# ----------------\n",
    "with tempfile.TemporaryDirectory() as td:\n",
    "    td_path = Path(td)\n",
    "    # create some test files\n",
    "    (td_path / 'a.csv').write_text('col1,col2\\n1,2')\n",
    "    (td_path / 'b.json').write_text('{\"x\":1}')\n",
    "    (td_path / '.hidden.txt').write_text('secret')\n",
    "    (td_path / 'c.jpg').write_text('not a data file')\n",
    "\n",
    "    # subdirectory test for recursive\n",
    "    sub = td_path / 'subdir'\n",
    "    sub.mkdir()\n",
    "    (sub / 'd.tsv').write_text('1\\t2')\n",
    "\n",
    "    # Non-recursive should find only top-level non-hidden data files\n",
    "    nonrec = list_data_files(td_path, recursive=False, include_hidden=False)\n",
    "    assert any(Path(p).name == 'a.csv' for p in nonrec), \"a.csv should be found (non-recursive)\"\n",
    "    assert any(Path(p).name == 'b.json' for p in nonrec), \"b.json should be found (non-recursive)\"\n",
    "    assert not any(Path(p).name == '.hidden.txt' for p in nonrec), \"hidden file should be excluded\"\n",
    "    assert not any(Path(p).name == 'd.tsv' for p in nonrec), \"subdir file should not be found in non-recursive mode\"\n",
    "\n",
    "    # Recursive should find files in subdirectories\n",
    "    rec = list_data_files(td_path, recursive=True, include_hidden=False)\n",
    "    assert any(Path(p).name == 'd.tsv' for p in rec), \"d.tsv should be found in recursive mode\"\n",
    "    assert all(Path(p).suffix.lower() in DATA_EXTENSIONS for p in rec), \"All returned files must have data extensions\"\n",
    "\n",
    "    # include_hidden=True should include hidden files\n",
    "    inc_hidden = list_data_files(td_path, recursive=False, include_hidden=True)\n",
    "    assert any(Path(p).name == '.hidden.txt' for p in inc_hidden), \"Hidden file should be included when include_hidden=True\"\n",
    "\n",
    "    print(\"Self-tests passed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efdcf03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 'titanic.xls' into df with shape (1309, 14)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Try the expected local filename first, fallback to searching known data files\n",
    "p = Path('titanic.xls')\n",
    "if not p.exists():\n",
    "    # use the helper list_data_files if available to find the file recursively\n",
    "    try:\n",
    "        candidates = [Path(f) for f in list_data_files('.', recursive=True, include_hidden=False)]\n",
    "    except Exception:\n",
    "        candidates = []\n",
    "    matches = [c for c in candidates if c.name.lower().startswith('titanic') and c.suffix.lower() in ('.xls', '.xlsx')]\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(\"titanic.xls not found in the current directory or subdirectories\")\n",
    "    p = matches[0]\n",
    "\n",
    "# Read the Excel file into a DataFrame named df\n",
    "df = pd.read_excel(p)\n",
    "\n",
    "# Self-tests\n",
    "assert isinstance(df, pd.DataFrame), \"Loaded object is not a pandas DataFrame\"\n",
    "assert df.shape[0] > 0, \"DataFrame appears to be empty\"\n",
    "print(f\"Loaded '{p}' into df with shape {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa4cb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.head():\n",
      "   pclass  survived  ...   body                        home.dest\n",
      "0       1         1  ...    NaN                     St Louis, MO\n",
      "1       1         1  ...    NaN  Montreal, PQ / Chesterville, ON\n",
      "2       1         0  ...    NaN  Montreal, PQ / Chesterville, ON\n",
      "3       1         0  ...  135.0  Montreal, PQ / Chesterville, ON\n",
      "4       1         0  ...    NaN  Montreal, PQ / Chesterville, ON\n",
      "\n",
      "[5 rows x 14 columns]\n",
      "\n",
      "df.info():\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 14 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   pclass     1309 non-null   int64  \n",
      " 1   survived   1309 non-null   int64  \n",
      " 2   name       1309 non-null   object \n",
      " 3   sex        1309 non-null   object \n",
      " 4   age        1046 non-null   float64\n",
      " 5   sibsp      1309 non-null   int64  \n",
      " 6   parch      1309 non-null   int64  \n",
      " 7   ticket     1309 non-null   object \n",
      " 8   fare       1308 non-null   float64\n",
      " 9   cabin      295 non-null    object \n",
      " 10  embarked   1307 non-null   object \n",
      " 11  boat       486 non-null    object \n",
      " 12  body       121 non-null    float64\n",
      " 13  home.dest  745 non-null    object \n",
      "dtypes: float64(3), int64(4), object(7)\n",
      "memory usage: 143.3+ KB\n",
      "\n",
      "df.describe(include='all'):\n",
      "             pclass     survived  ...        body     home.dest\n",
      "count   1309.000000  1309.000000  ...  121.000000           745\n",
      "unique          NaN          NaN  ...         NaN           369\n",
      "top             NaN          NaN  ...         NaN  New York, NY\n",
      "freq            NaN          NaN  ...         NaN            64\n",
      "mean       2.294882     0.381971  ...  160.809917           NaN\n",
      "std        0.837836     0.486055  ...   97.696922           NaN\n",
      "min        1.000000     0.000000  ...    1.000000           NaN\n",
      "25%        2.000000     0.000000  ...   72.000000           NaN\n",
      "50%        3.000000     0.000000  ...  155.000000           NaN\n",
      "75%        3.000000     1.000000  ...  256.000000           NaN\n",
      "max        3.000000     1.000000  ...  328.000000           NaN\n",
      "\n",
      "[11 rows x 14 columns]\n",
      "\n",
      "df.isnull().sum():\n",
      "pclass          0\n",
      "survived        0\n",
      "name            0\n",
      "sex             0\n",
      "age           263\n",
      "sibsp           0\n",
      "parch           0\n",
      "ticket          0\n",
      "fare            1\n",
      "cabin        1014\n",
      "embarked        2\n",
      "boat          823\n",
      "body         1188\n",
      "home.dest     564\n",
      "dtype: int64\n",
      "\n",
      "Self-tests passed."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Quick inspection outputs\n",
    "print(\"df.head():\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\ndf.info():\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\ndf.describe(include='all'):\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "print(\"\\ndf.isnull().sum():\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Self-tests\n",
    "assert isinstance(df, pd.DataFrame), \"df is not a pandas DataFrame\"\n",
    "assert df.shape[0] > 0, \"DataFrame appears to be empty\"\n",
    "desc = df.describe(include='all')\n",
    "assert isinstance(desc, pd.DataFrame), \"describe(...) did not return a DataFrame\"\n",
    "nulls = df.isnull().sum()\n",
    "assert isinstance(nulls, pd.Series), \"isnull().sum() did not return a Series\"\n",
    "print(\"\\nSelf-tests passed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5248b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete. Resulting df shape: (1309, 28)\n",
      "\n",
      "df.head():\n",
      "   pclass  survived  ... Embarked_Q  Embarked_S\n",
      "0       1         1  ...      False        True\n",
      "1       1         1  ...      False        True\n",
      "2       1         0  ...      False        True\n",
      "3       1         0  ...      False        True\n",
      "4       1         0  ...      False        True\n",
      "\n",
      "[5 rows x 28 columns]\n",
      "\n",
      "Remaining null counts (top 10):\n",
      "body         1188\n",
      "cabin        1014\n",
      "boat          823\n",
      "home.dest     564\n",
      "fare            1\n",
      "name            0\n",
      "survived        0\n",
      "pclass          0\n",
      "age             0\n",
      "parch           0\n",
      "dtype: int64"
     ]
    }
   ],
   "source": [
    "# Data cleaning & preprocessing for Titanic df\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Work on a copy to avoid accidental side-effects\n",
    "df_clean = df.copy()\n",
    "\n",
    "# 1) Impute Embarked: fill missing with mode\n",
    "if 'embarked' in df_clean.columns:\n",
    "    if df_clean['embarked'].isnull().any():\n",
    "        mode_emb = df_clean['embarked'].mode(dropna=True)\n",
    "        if not mode_emb.empty:\n",
    "            df_clean['embarked'] = df_clean['embarked'].fillna(mode_emb[0])\n",
    "        else:\n",
    "            # fallback to a placeholder if for some reason mode is empty\n",
    "            df_clean['embarked'] = df_clean['embarked'].fillna('U')\n",
    "\n",
    "# 2) Create FamilySize (as requested: SibSp + Parch) and a simple IsAlone flag\n",
    "if {'sibsp', 'parch'}.issubset(df_clean.columns):\n",
    "    df_clean['FamilySize'] = df_clean['sibsp'] + df_clean['parch']\n",
    "    df_clean['IsAlone'] = (df_clean['FamilySize'] == 0).astype(int)\n",
    "else:\n",
    "    raise KeyError(\"Expected 'sibsp' and 'parch' columns to create FamilySize\")\n",
    "\n",
    "# 3) Impute Age using median age by (sex, pclass) — a common pragmatic approach\n",
    "if 'age' in df_clean.columns:\n",
    "    # compute group medians\n",
    "    group_median = df_clean.groupby(['sex', 'pclass'])['age'].transform('median')\n",
    "    df_clean['age'] = df_clean['age'].fillna(group_median)\n",
    "    # if any ages still missing (group median could be NaN), fallback to overall median\n",
    "    if df_clean['age'].isnull().any():\n",
    "        overall_median = df_clean['age'].median()\n",
    "        df_clean['age'] = df_clean['age'].fillna(overall_median)\n",
    "\n",
    "# 4) Cabin handling: create CabinKnown flag and extract cabin deck (first letter) as a categorical feature\n",
    "if 'cabin' in df_clean.columns:\n",
    "    df_clean['CabinKnown'] = df_clean['cabin'].notna().astype(int)\n",
    "    # extract deck letter; fillna with 'U' (Unknown) then map 'U' -> 'Unknown' label\n",
    "    df_clean['CabinDeck'] = df_clean['cabin'].fillna('U').astype(str).str.strip().str[0]\n",
    "    df_clean['CabinDeck'] = df_clean['CabinDeck'].replace({'U': 'Unknown'})\n",
    "    # Optionally convert to dummies — keep them for modeling\n",
    "    deck_dummies = pd.get_dummies(df_clean['CabinDeck'], prefix='Deck')\n",
    "    df_clean = pd.concat([df_clean, deck_dummies], axis=1)\n",
    "else:\n",
    "    # If no cabin column, ensure CabinKnown exists for consistency\n",
    "    df_clean['CabinKnown'] = 0\n",
    "\n",
    "# 5) Convert categorical Sex to numeric (male = 1, female = 0). Drop original sex to avoid duplication\n",
    "if 'sex' in df_clean.columns:\n",
    "    df_clean['sex_male'] = df_clean['sex'].astype(str).str.lower().map({'male': 1, 'female': 0})\n",
    "    # If there are any unexpected values, fill with 0 (treat as female/unknown -> 0)\n",
    "    df_clean['sex_male'] = df_clean['sex_male'].fillna(0).astype(int)\n",
    "    # drop original sex column\n",
    "    df_clean = df_clean.drop(columns=['sex'])\n",
    "\n",
    "# 6) Convert Embarked to numeric dummies and drop original\n",
    "if 'embarked' in df_clean.columns:\n",
    "    embarked_dummies = pd.get_dummies(df_clean['embarked'], prefix='Embarked')\n",
    "    df_clean = pd.concat([df_clean, embarked_dummies], axis=1)\n",
    "    df_clean = df_clean.drop(columns=['embarked'])\n",
    "\n",
    "# 7) Drop irrelevant columns as requested: Ticket (and PassengerId if present). Keep name by default.\n",
    "cols_to_drop = []\n",
    "if 'ticket' in df_clean.columns:\n",
    "    cols_to_drop.append('ticket')\n",
    "if 'passengerid' in (c.lower() for c in df_clean.columns):\n",
    "    # find exact column name (case-insensitive)\n",
    "    pid_col = [c for c in df_clean.columns if c.lower() == 'passengerid'][0]\n",
    "    cols_to_drop.append(pid_col)\n",
    "\n",
    "if cols_to_drop:\n",
    "    df_clean = df_clean.drop(columns=cols_to_drop)\n",
    "\n",
    "# Optional: reorder columns to put newly created features near front (not required)\n",
    "# ensure df variable is updated for downstream cells\n",
    "df = df_clean\n",
    "\n",
    "# ----------------\n",
    "# Self-tests / quick checks\n",
    "# ----------------\n",
    "# 1) Age should have no missing values after imputation\n",
    "assert 'age' in df.columns, \"'age' column missing after processing\"\n",
    "assert df['age'].isnull().sum() == 0, f\"There are still missing ages: {df['age'].isnull().sum()}\"\n",
    "\n",
    "# 2) Embarked should have been converted to dummy columns (or original filled if no conversion)\n",
    "embarked_cols = [c for c in df.columns if c.startswith('Embarked_')]\n",
    "assert len(embarked_cols) > 0 or 'embarked' not in df.columns, \"Embarked not handled correctly\"\n",
    "\n",
    "# 3) sex_male created and is binary\n",
    "assert 'sex_male' in df.columns, \"sex_male column was not created\"\n",
    "assert set(df['sex_male'].unique()) <= {0, 1}, \"sex_male contains values other than 0/1\"\n",
    "\n",
    "# 4) FamilySize exists and matches sibsp+parch\n",
    "assert 'FamilySize' in df.columns, \"FamilySize column missing\"\n",
    "assert (df['FamilySize'] == (df['sibsp'] + df['parch'])).all(), \"FamilySize does not equal sibsp + parch\"\n",
    "\n",
    "# 5) Ticket should be dropped if it existed\n",
    "assert 'ticket' not in df.columns, \"ticket column was not dropped\"\n",
    "\n",
    "print('Cleaning complete. Resulting df shape:', df.shape)\n",
    "print('\\ndf.head():')\n",
    "print(df.head())\n",
    "\n",
    "# brief summary of nulls to confirm\n",
    "print('\\nRemaining null counts (top 10):')\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab94321a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline RandomForest (test set) — accuracy: 0.7977, precision: 0.7282, recall: 0.7500\n",
      "Confusion matrix:\n",
      " [[134  28]\n",
      " [ 25  75]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.83      0.83       162\n",
      "           1       0.73      0.75      0.74       100\n",
      "\n",
      "    accuracy                           0.80       262\n",
      "   macro avg       0.79      0.79      0.79       262\n",
      "weighted avg       0.80      0.80      0.80       262\n",
      "\n",
      "CV test_accuracy: mean=0.7861, std=0.0319, scores=[0.72519084 0.81679389 0.79007634 0.80534351 0.79310345]\n",
      "CV test_precision: mean=0.7302, std=0.0487, scores=[0.6372549  0.76530612 0.73684211 0.73786408 0.77380952]\n",
      "CV test_recall: mean=0.7020, std=0.0471, scores=[0.65 0.75 0.7  0.76 0.65]\n",
      "CV test_f1: mean=0.7149, std=0.0403, scores=[0.64356436 0.75757576 0.71794872 0.74876847 0.70652174]\n",
      "\n",
      "Self-tests passed. Baseline training & evaluation complete."
     ]
    }
   ],
   "source": [
    "# Baseline model: train/test split, RandomForest baseline, evaluation + cross-validation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
    "\n",
    "# --- Preconditions / quick checks ---\n",
    "assert 'survived' in df.columns, \"Expected target column 'survived' in df\"\n",
    "\n",
    "# Select numeric and boolean feature columns automatically (exclude target)\n",
    "feature_cols = df.select_dtypes(include=[np.number, 'bool']).columns.tolist()\n",
    "feature_cols = [c for c in feature_cols if c != 'survived']\n",
    "\n",
    "# If automatic selection yields too few features (e.g., only target), fall back to a small hand-picked set\n",
    "if len(feature_cols) < 3:\n",
    "    fallback = [c for c in ['pclass', 'age', 'fare', 'sex_male', 'FamilySize', 'IsAlone', 'CabinKnown'] if c in df.columns]\n",
    "    if not fallback:\n",
    "        raise RuntimeError('No reasonable numeric features found for modeling')\n",
    "    feature_cols = fallback\n",
    "\n",
    "# Prepare X and y\n",
    "X = df[feature_cols].copy()\n",
    "# convert any boolean columns to int so the model sees 0/1\n",
    "bool_cols = X.select_dtypes(include='bool').columns.tolist()\n",
    "if bool_cols:\n",
    "    X[bool_cols] = X[bool_cols].astype(int)\n",
    "\n",
    "y = df['survived']\n",
    "\n",
    "# Train/test split (stratified by target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Build preprocessing + classifier pipeline\n",
    "preprocessor = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', clf)\n",
    "])\n",
    "\n",
    "# Fit pipeline on training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on test set\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation metrics on the test set\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "print(f\"Baseline RandomForest (test set) — accuracy: {acc:.4f}, precision: {prec:.4f}, recall: {rec:.4f}\")\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "print(\"\\nClassification report:\\n\", report)\n",
    "\n",
    "# Cross-validation (5-fold stratified) to estimate baseline stability\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = cross_validate(\n",
    "    pipe, X, y, cv=cv,\n",
    "    scoring=['accuracy', 'precision', 'recall', 'f1'],\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "# Summarize CV results\n",
    "for metric in ['test_accuracy', 'test_precision', 'test_recall', 'test_f1']:\n",
    "    vals = cv_results[metric]\n",
    "    print(f\"CV {metric}: mean={np.nanmean(vals):.4f}, std={np.nanstd(vals):.4f}, scores={vals}\")\n",
    "\n",
    "# ----------------\n",
    "# Self-tests (simple checks to catch obvious issues)\n",
    "# ----------------\n",
    "# 1) Ensure training split is non-empty\n",
    "assert X_train.shape[0] > 0, \"X_train is empty\"\n",
    "assert y_train.shape[0] == X_train.shape[0], \"Mismatch between X_train and y_train sizes\"\n",
    "\n",
    "# 2) Ensure preprocessing removed NaNs when applied to training data\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "assert np.isfinite(X_train_transformed).all(), \"Preprocessed X_train contains non-finite values\"\n",
    "\n",
    "# 3) Model produced predictions of expected length\n",
    "assert y_pred.shape[0] == X_test.shape[0], \"Prediction length does not match X_test\"\n",
    "\n",
    "# 4) Basic sanity on metrics\n",
    "assert 0.0 <= acc <= 1.0, \"Accuracy out of [0,1]\"\n",
    "assert 0.0 <= prec <= 1.0, \"Precision out of [0,1]\"\n",
    "assert 0.0 <= rec <= 1.0, \"Recall out of [0,1]\"\n",
    "\n",
    "print('\\nSelf-tests passed. Baseline training & evaluation complete.')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
